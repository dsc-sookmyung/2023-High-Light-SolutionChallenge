{
	"page_id": 37,
	"full_text": {
		"audio_url": "",
		"full_text": "4. Feature Selection\n▪ Another way to reduce the dimensionality is to use only a subset\nof the features\n– We would not lose information if redundant and irrelevant features are \npresent\n▪ Redundant features\n– Duplicate much or all of the information contained in other attributes\n– (ex) the price of a product  the amount of sales tax\n▪ Irrelevant features\n– Contain almost no useful information for the data mining task\n– (ex) ‘student ID’ for the task of predicting students’ GPA\n37\n"
	},
	"text": [
		{
			"audio_url": "",
			"font_size": 32,
			"text": "4. Feature Selection\n"
		},
		{
			"audio_url": "",
			"font_size": 24,
			"text": "▪ Another way to reduce the dimensionality is to use only a subset\nof the features\n"
		},
		{
			"audio_url": "",
			"font_size": 20,
			"text": "– We would not lose information if redundant and irrelevant features are \npresent\n"
		},
		{
			"audio_url": "",
			"font_size": 24,
			"text": "▪ Redundant features\n"
		},
		{
			"audio_url": "",
			"font_size": 20,
			"text": "– Duplicate much or all of the information contained in other attributes\n– (ex) the price of a product  the amount of sales tax\n"
		},
		{
			"audio_url": "",
			"font_size": 24,
			"text": "▪ Irrelevant features\n"
		},
		{
			"audio_url": "",
			"font_size": 20,
			"text": "– Contain almost no useful information for the data mining task\n– (ex) ‘student ID’ for the task of predicting students’ GPA\n"
		},
		{
			"audio_url": "",
			"font_size": 14,
			"text": "37\n"
		}
	],
	"image": {
		"img_idx": 1,
		"audio_url": "",
		"img_url": ""
	}
}