{
	"page_id": 69,
	"full_text": {
		"audio_url": "",
		"full_text": "Definition: Mutual Information (1/2)\n▪ Consider two sets of values, X and Y, which occur in pairs (X, Y)\nX\nY\n(1, 2, 3, 1, 3)\n(2, 3, 1, 2, 2) \n(X, Y)\n((1, 2), (2, 3), (3, 1), (1, 2), (3, 2))\n▪ First, we measure the average information (i.e., entropy) of X, Y, \nand (X, Y), respectively\n69\n"
	},
	"text": [
		{
			"audio_url": "",
			"font_size": 32,
			"text": "Definition: Mutual Information (1/2)\n"
		},
		{
			"audio_url": "",
			"font_size": 24,
			"text": "▪ Consider two sets of values, X and Y, which occur in pairs (X, Y)\n"
		},
		{
			"audio_url": "",
			"font_size": 20,
			"text": "X\nY\n(1, 2, 3, 1, 3)\n(2, 3, 1, 2, 2) \n(X, Y)\n((1, 2), (2, 3), (3, 1), (1, 2), (3, 2))\n"
		},
		{
			"audio_url": "",
			"font_size": 24,
			"text": "▪ First, we measure the average information (i.e., entropy) of X, Y, \nand (X, Y), respectively\n"
		},
		{
			"audio_url": "",
			"font_size": 14,
			"text": "69\n"
		}
	],
	"image": {
		"img_idx": 1,
		"audio_url": "",
		"img_url": ""
	}
}