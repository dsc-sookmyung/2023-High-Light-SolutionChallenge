{
	"page_id": 68,
	"full_text": {
		"audio_url": "",
		"full_text": "Entropy\nâ–ª Measure the average information in a single set of values\nğ‘š\nğ» ğ‘‹ = à·\nğ‘—=1\nğ‘š\nğ‘ƒ ğ‘‹ = ğ‘¢ğ‘— ğ¼ ğ‘‹ = ğ‘¢ğ‘— = âˆ’ à·\nğ‘—=1\nğ‘ƒ ğ‘‹ = ğ‘¢ğ‘— log2 ğ‘ƒ ğ‘‹ = ğ‘¢ğ‘—\nâ€“ X: a set of values with m distinct values u1, u2, â€¦, um\nâ€“ H(X): the entropy of X\nâ€“ P(X = uj): the probability of uj in X\nâ€“ I(X = uj): the amount of information acquired through observing uj\nâ€¢ I(X = uj) = log2(1/P(X = uj)) = â€“log2P(X = uj)\nâ€¢ As P(X = uj) increases, I(X = uj) decreases, and vice versa\nEntropy = 0\nEntropy = 0.81\nEntropy = 1\n68\n"
	},
	"text": [
		{
			"audio_url": "",
			"font_size": 32,
			"text": "Entropy\n"
		},
		{
			"audio_url": "",
			"font_size": 24,
			"text": "â–ª Measure the average information in a single set of values\n"
		},
		{
			"audio_url": "",
			"font_size": 13,
			"text": "ğ‘š\n"
		},
		{
			"audio_url": "",
			"font_size": 18,
			"text": "ğ» ğ‘‹ = à·\n"
		},
		{
			"audio_url": "",
			"font_size": 13,
			"text": "ğ‘—=1\nğ‘š\n"
		},
		{
			"audio_url": "",
			"font_size": 18,
			"text": "ğ‘ƒ ğ‘‹ = ğ‘¢ğ‘— ğ¼ ğ‘‹ = ğ‘¢ğ‘— = âˆ’ à·\n"
		},
		{
			"audio_url": "",
			"font_size": 13,
			"text": "ğ‘—=1\nğ‘ƒ ğ‘‹ = ğ‘¢ğ‘— log2 ğ‘ƒ ğ‘‹ = ğ‘¢ğ‘—\nâ€“ X: a set of values with m distinct values u1, u2, â€¦, um\n"
		},
		{
			"audio_url": "",
			"font_size": 20,
			"text": "â€“ H(X): the entropy of X\nâ€“ P(X = uj): the probability of uj in X\n"
		},
		{
			"audio_url": "",
			"font_size": 13,
			"text": "â€“ I(X = uj): the amount of information acquired through observing uj\n"
		},
		{
			"audio_url": "",
			"font_size": 18,
			"text": "â€¢ I(X = uj) = log2(1/P(X = uj)) = â€“log2P(X = uj)\nâ€¢ As P(X = uj) increases, I(X = uj) decreases, and vice versa\nEntropy = 0\nEntropy = 0.81\nEntropy = 1\n"
		},
		{
			"audio_url": "",
			"font_size": 14,
			"text": "68\n"
		}
	],
	"image": {
		"img_idx": 1,
		"audio_url": "",
		"img_url": ""
	}
}