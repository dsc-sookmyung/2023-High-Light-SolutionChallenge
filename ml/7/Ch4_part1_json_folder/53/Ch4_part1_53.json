{"page_id": 53, "full_text": {"audio_url": "https://storage.googleapis.com/cloud_storage_leturn/Ch4_part1.pdf/Ch4_part1_audio_folder/53/Ch4_part1_full_audio_53.mp3", "full_text": "Logistic Regression\n\u25aa Suppose that the target attribute is y \uf0ce {0, 1}\n\u2013 1: positive class (e.g., malignant, spam, fraudulent)\n\u2013 0: negative class (e.g., benign, non-spam, normal)\n\u25aa Basic idea\n\u2013 Suppose we are given a data instance x = (x1, x2, \u2026, xn)\n\u2013 We model the probability that y = 1 given x using the logistic function:\n\ud835\udc43(\ud835\udc66 = 1|\ud835\udc31) =\n1\n1 + \ud835\udc52\u2212(\ud835\udc640+\ud835\udc641\ud835\udc651+\ud835\udc642\ud835\udc652+\u22ef+\ud835\udc64\ud835\udc5b\ud835\udc65\ud835\udc5b)\n\u2013 We learn the parameters w0, w1, w2, \u2026, wn from the training data\n\u2013 If P(y = 1|x) \u2265 0.5, then we classify x as y = 1, otherwise y = 0 \n53\n"}, "text": [{"audio_url": "https://storage.googleapis.com/cloud_storage_leturn/Ch4_part1.pdf/Ch4_part1_audio_folder/53/Ch4_part1_audio_53_1.mp3", "font_size": 32, "text": "Logistic Regression\n"}, {"audio_url": "https://storage.googleapis.com/cloud_storage_leturn/Ch4_part1.pdf/Ch4_part1_audio_folder/53/Ch4_part1_audio_53_2.mp3", "font_size": 24, "text": "\u25aa Suppose that the target attribute is y \uf0ce {0, 1}\n"}, {"audio_url": "https://storage.googleapis.com/cloud_storage_leturn/Ch4_part1.pdf/Ch4_part1_audio_folder/53/Ch4_part1_audio_53_3.mp3", "font_size": 20, "text": "\u2013 1: positive class (e.g., malignant, spam, fraudulent)\n\u2013 0: negative class (e.g., benign, non-spam, normal)\n"}, {"audio_url": "https://storage.googleapis.com/cloud_storage_leturn/Ch4_part1.pdf/Ch4_part1_audio_folder/53/Ch4_part1_audio_53_4.mp3", "font_size": 24, "text": "\u25aa Basic idea\n"}, {"audio_url": "https://storage.googleapis.com/cloud_storage_leturn/Ch4_part1.pdf/Ch4_part1_audio_folder/53/Ch4_part1_audio_53_5.mp3", "font_size": 20, "text": "\u2013 Suppose we are given a data instance x = (x1, x2, \u2026, xn)\n\u2013 We model the probability that y = 1 given x using the logistic function:\n"}, {"audio_url": "https://storage.googleapis.com/cloud_storage_leturn/Ch4_part1.pdf/Ch4_part1_audio_folder/53/Ch4_part1_audio_53_6.mp3", "font_size": 24, "text": "\ud835\udc43(\ud835\udc66 = 1|\ud835\udc31) =\n1\n"}, {"audio_url": "https://storage.googleapis.com/cloud_storage_leturn/Ch4_part1.pdf/Ch4_part1_audio_folder/53/Ch4_part1_audio_53_7.mp3", "font_size": 18, "text": "1 + \ud835\udc52\u2212(\ud835\udc640+\ud835\udc641\ud835\udc651+\ud835\udc642\ud835\udc652+\u22ef+\ud835\udc64\ud835\udc5b\ud835\udc65\ud835\udc5b)\n"}, {"audio_url": "https://storage.googleapis.com/cloud_storage_leturn/Ch4_part1.pdf/Ch4_part1_audio_folder/53/Ch4_part1_audio_53_8.mp3", "font_size": 20, "text": "\u2013 We learn the parameters w0, w1, w2, \u2026, wn from the training data\n\u2013 If P(y = 1|x) \u2265 0.5, then we classify x as y = 1, otherwise y = 0 \n"}, {"audio_url": "https://storage.googleapis.com/cloud_storage_leturn/Ch4_part1.pdf/Ch4_part1_audio_folder/53/Ch4_part1_audio_53_9.mp3", "font_size": 14, "text": "53\n"}], "image": [{"img_idx": 1, "img_url": "https://storage.googleapis.com/cloud_storage_leturn/Ch4_part1.pdf/Ch4_part1_image_folder/53/Ch4_part1_image_1.png", "img_text": "a graph with a curve and a line of the same length", "img_audio_url": "https://storage.googleapis.com/cloud_storage_leturn/Ch4_part1.pdf/Ch4_part1_image_folder/53/Ch4_part1_image_audio_1.mp3"}]}