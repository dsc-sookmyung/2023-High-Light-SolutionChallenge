{
	"page_id": 68,
	"full_text": {
		"audio_url": "",
		"full_text": "Entropy\n▪ Measure the average information in a single set of values\n𝑚\n𝐻 𝑋 = ෍\n𝑗=1\n𝑚\n𝑃 𝑋 = 𝑢𝑗 𝐼 𝑋 = 𝑢𝑗 = − ෍\n𝑗=1\n𝑃 𝑋 = 𝑢𝑗 log2 𝑃 𝑋 = 𝑢𝑗\n– X: a set of values with m distinct values u1, u2, …, um\n– H(X): the entropy of X\n– P(X = uj): the probability of uj in X\n– I(X = uj): the amount of information acquired through observing uj\n• I(X = uj) = log2(1/P(X = uj)) = –log2P(X = uj)\n• As P(X = uj) increases, I(X = uj) decreases, and vice versa\nEntropy = 0\nEntropy = 0.81\nEntropy = 1\n68\n"
	},
	"text": [
		{
			"audio_url": "",
			"font_size": 32,
			"text": "Entropy\n"
		},
		{
			"audio_url": "",
			"font_size": 24,
			"text": "▪ Measure the average information in a single set of values\n"
		},
		{
			"audio_url": "",
			"font_size": 13,
			"text": "𝑚\n"
		},
		{
			"audio_url": "",
			"font_size": 18,
			"text": "𝐻 𝑋 = ෍\n"
		},
		{
			"audio_url": "",
			"font_size": 13,
			"text": "𝑗=1\n𝑚\n"
		},
		{
			"audio_url": "",
			"font_size": 18,
			"text": "𝑃 𝑋 = 𝑢𝑗 𝐼 𝑋 = 𝑢𝑗 = − ෍\n"
		},
		{
			"audio_url": "",
			"font_size": 13,
			"text": "𝑗=1\n𝑃 𝑋 = 𝑢𝑗 log2 𝑃 𝑋 = 𝑢𝑗\n– X: a set of values with m distinct values u1, u2, …, um\n"
		},
		{
			"audio_url": "",
			"font_size": 20,
			"text": "– H(X): the entropy of X\n– P(X = uj): the probability of uj in X\n"
		},
		{
			"audio_url": "",
			"font_size": 13,
			"text": "– I(X = uj): the amount of information acquired through observing uj\n"
		},
		{
			"audio_url": "",
			"font_size": 18,
			"text": "• I(X = uj) = log2(1/P(X = uj)) = –log2P(X = uj)\n• As P(X = uj) increases, I(X = uj) decreases, and vice versa\nEntropy = 0\nEntropy = 0.81\nEntropy = 1\n"
		},
		{
			"audio_url": "",
			"font_size": 14,
			"text": "68\n"
		}
	],
	"image": {
		"img_idx": 1,
		"audio_url": "",
		"img_url": ""
	}
}