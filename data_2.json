{
	"1": {
		"page_id": 1,
		"full_text": {
			"audio_url": "",
			"full_text": "Data\nChapter 2\n1\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 40,
				"text": "Chapter 2\n"
			},
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Data\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "1\n"
			}
		]
	},
	"2": {
		"page_id": 2,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Data object: an entity with measurable properties\n– Also called record, point, vector, case, sample, instance, observation, …\n▪ Attribute: a property or characteristic of a data object\n– Also called variable, field, feature, dimension, …\n▪ Data set: a collection of data objects\n– Commonly stored in flat files or database tables \nTerminology\n2\nStudent ID\nYear\nGPA\n…\n1042129\nJunior\n3.85\n…\n1034262\nSenior\n3.24\n…\n1052663\nSophomore\n3.51\n…\n1082246\nFreshman\n3.62\n…\nData set\nData\nobject\nAttribute\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Terminology\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "Data set\nStudent ID\n1042129\n1034262\n1052663\n1082246\nYear\nJunior\nSenior\nSophomore\nFreshman\nGPA\n3.85\n3.24\n3.51\n3.62\n…\n…\n…\n…\n…\nAttribute\nData\nobject\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Data object: an entity with measurable properties\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Also called record, point, vector, case, sample, instance, observation, …\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Attribute: a property or characteristic of a data object\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Also called variable, field, feature, dimension, …\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Data set: a collection of data objects\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Commonly stored in flat files or database tables \n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "2\n"
			}
		]
	},
	"3": {
		"page_id": 3,
		"full_text": {
			"audio_url": "",
			"full_text": "1.  The types of data\n– The attributes can be of different types\n• (ex) categorical (city, gender, genre, …), numeric (temperature, age, price, …)\n– Data sets often have different characteristics\n• (ex) record data, graph data (social network), ordered data (time series), …\n– The type of data determines which methods and techniques can be used\n2.  The quality of the data\n– Data is often far from perfect\n• (ex) noise, outliers, missing data, inconsistent data, duplicate data\n• (ex) biased or unrepresentative data\n– Understanding and improving data quality typically improves the quality \nof the resulting analysis\nData-Related Issues for Data Mining (1/2)\n3\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Data-Related Issues for Data Mining (1/2)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "1.  The types of data\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– The attributes can be of different types\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• (ex) categorical (city, gender, genre, …), numeric (temperature, age, price, …)\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Data sets often have different characteristics\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• (ex) record data, graph data (social network), ordered data (time series), …\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– The type of data determines which methods and techniques can be used\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "2.  The quality of the data\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Data is often far from perfect\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• (ex) noise, outliers, missing data, inconsistent data, duplicate data\n• (ex) biased or unrepresentative data\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Understanding and improving data quality typically improves the quality \nof the resulting analysis\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "3\n"
			}
		]
	},
	"4": {
		"page_id": 4,
		"full_text": {
			"audio_url": "",
			"full_text": "3.  Preprocessing \n– Often, the raw data must be processed to make it suitable for analysis\n• (ex) continuous attribute (e.g., length) → categorical attribute (e.g., S/M/L)\n• (ex) dimensionality reduction (e.g., 100 attributes → 10 attributes)\n– The goal is to modify the data so that it better fits a specific technique\n4.  Measures of similarity\n– Data mining tasks often need to measure the similarity between objects\n• (ex) clustering, classification, or anomaly detection\n– There are many similarity or distance measures\n• The proper choice depends on the type of data and the particular application\nData-Related Issues for Data Mining (2/2)\n4\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Data-Related Issues for Data Mining (2/2)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "3.  Preprocessing \n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Often, the raw data must be processed to make it suitable for analysis\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• (ex) continuous attribute (e.g., length) → categorical attribute (e.g., S/M/L)\n• (ex) dimensionality reduction (e.g., 100 attributes → 10 attributes)\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– The goal is to modify the data so that it better fits a specific technique\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "4.  Measures of similarity\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Data mining tasks often need to measure the similarity between objects\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• (ex) clustering, classification, or anomaly detection\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– There are many similarity or distance measures\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• The proper choice depends on the type of data and the particular application\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "4\n"
			}
		]
	},
	"5": {
		"page_id": 5,
		"full_text": {
			"audio_url": "",
			"full_text": "Types of Data\n5\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 40,
				"text": "Types of Data\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "5\n"
			}
		]
	},
	"6": {
		"page_id": 6,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Categorical (qualitative) attribute\n– An attribute that can take on one of a limited number of possible values\n• (ex) zip code, student ID, city\n– Lacks most of the properties of numbers and should be treated as symbols\n•\n(ex) ‘Junior’ + ‘Senior’ (X)\n– However, the values may have an order relationship (e.g., ‘S’ < ‘M’ < ‘L’)\n▪ Numeric (quantitative) attribute\n– An attribute whose value can be any number from a defined range\n• (ex) temperature, age, mass, length, counts\n– Has most of the properties of numbers (e.g., 35.1C < 40.2C (O))\n– Associated with a measurement scale (e.g., C, F, cm, kg, GB)\n1. Types of Attributes\n6\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "1. Types of Attributes\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Categorical (qualitative) attribute\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– An attribute that can take on one of a limited number of possible values\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• (ex) zip code, student ID, city\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Lacks most of the properties of numbers and should be treated as symbols\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "•\n(ex) ‘Junior’ + ‘Senior’ (X)\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– However, the values may have an order relationship (e.g., ‘S’ < ‘M’ < ‘L’)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Numeric (quantitative) attribute\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– An attribute whose value can be any number from a defined range\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• (ex) temperature, age, mass, length, counts\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Has most of the properties of numbers (e.g., 35.1C < 40.2C (O))\n– Associated with a measurement scale (e.g., C, F, cm, kg, GB)\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "6\n"
			}
		]
	},
	"7": {
		"page_id": 7,
		"full_text": {
			"audio_url": "",
			"full_text": "Attribute Type\nDescription\nExamples\nCategorical (qualitative)\nNominal\nThe values are just different \nnames\n(=, ≠)\nzip codes, \nemployee IDs, \neye color, \ngender\nOrdinal\nThe values provide enough \ninformation to order objects\n(<, >)\nhardness of minerals, \n{good, better, best}, \ngrades, \nstreet numbers\nNumeric (quantitative)\nThe values are represented by \nnumbers (e.g., real numbers, \nintegers)\n(+, −, ×, /)\ntemperature,\nmonetary quantities, \ncounts,\nage, \nmass, \nlength, \nelectrical current\nDifferent Attribute Types\n7\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Different Attribute Types\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "Attribute Type\nDescription\nExamples\n"
			},
			{
				"audio_url": "",
				"font_size": 16,
				"text": "Categorical (qualitative)\nNominal\nOrdinal\nThe values are just different \nnames\n(=, ≠)\nThe values provide enough \ninformation to order objects\n(<, >)\nNumeric (quantitative)\nThe values are represented by \nnumbers (e.g., real numbers, \nintegers)\n(+, −, ×, /)\nzip codes, \nemployee IDs, \neye color, \ngender\nhardness of minerals, \n{good, better, best}, \ngrades, \nstreet numbers\ntemperature,\nmonetary quantities, \ncounts,\nage, \nmass, \nlength, \nelectrical current\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "7\n"
			}
		]
	},
	"8": {
		"page_id": 8,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Discrete attribute\n– Has a finite or countably infinite set of values (e.g., 1, 2, 3, …)\n– Categorical (e.g., zip codes) or numeric (e.g., counts)\n– Often represented using integer variables\n– Binary attribute: a special case with only two values\n• (ex) true/false, yes/no, 0/1\n▪ Continuous attribute\n– One whose values are real numbers (i.e., can take any value)\n• (ex) temperature, height, weight\n– Typically represented as floating-point variables\nAnother Way to Distinguish Attributes\n8\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Another Way to Distinguish Attributes\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Discrete attribute\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Has a finite or countably infinite set of values (e.g., 1, 2, 3, …)\n– Categorical (e.g., zip codes) or numeric (e.g., counts)\n– Often represented using integer variables\n– Binary attribute: a special case with only two values\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• (ex) true/false, yes/no, 0/1\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Continuous attribute\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– One whose values are real numbers (i.e., can take any value)\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• (ex) temperature, height, weight\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Typically represented as floating-point variables\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "8\n"
			}
		]
	},
	"9": {
		"page_id": 9,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ There are many types of data sets\n– As the field of data mining develops and matures, a greater variety of \ndata sets become available for analysis\n▪ We focus on some of the most common types:\n(1) Record data\n(2) Graph-based data\n(3) Ordered data\n▪ However, these categories do not cover all possibilities and \nother types are certainly possible\n2. Types of Data Sets\n9\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "2. Types of Data Sets\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ There are many types of data sets\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– As the field of data mining develops and matures, a greater variety of \ndata sets become available for analysis\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ We focus on some of the most common types:\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "(1) Record data\n(2) Graph-based data\n(3) Ordered data\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ However, these categories do not cover all possibilities and \nother types are certainly possible\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "9\n"
			}
		]
	},
	"10": {
		"page_id": 10,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Dimensionality\n– The number of attributes in the data set\n– The curse of dimensionality\n• The difficulties associated with high-dimensional data\n• Because of this, dimensionality reduction is often used\n▪ Distribution\n– The frequency of various values for the attributes\n• (ex) Gaussian (normal) distribution\n– However, many data sets have distributions that are \nnot well captured by standard statistical distributions\n– Skewness in the distribution can make mining difficult \n• (ex) Male : Female = 5 : 95\nGeneral Characteristics of Data Sets\n10\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "General Characteristics of Data Sets\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Dimensionality\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– The number of attributes in the data set\n– The curse of dimensionality\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• The difficulties associated with high-dimensional data\n• Because of this, dimensionality reduction is often used\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Distribution\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– The frequency of various values for the attributes\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• (ex) Gaussian (normal) distribution\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– However, many data sets have distributions that are \nnot well captured by standard statistical distributions\n– Skewness in the distribution can make mining difficult \n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• (ex) Male : Female = 5 : 95\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "10\n"
			}
		]
	},
	"11": {
		"page_id": 11,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ The data set is a collection of records (data objects)\n– Each record consists of a fixed set of fields (attributes)\n▪ There is no explicit relationship among records or fields\n▪ Usually stored either in flat files or in relational databases\n– However, data mining often does not use any of the additional \ninformation available in a relational database\n– Rather, the database serves as a convenient place to find records\n(1) Record Data\n11\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "(1) Record Data\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ The data set is a collection of records (data objects)\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Each record consists of a fixed set of fields (attributes)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ There is no explicit relationship among records or fields\n▪ Usually stored either in flat files or in relational databases\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– However, data mining often does not use any of the additional \ninformation available in a relational database\n– Rather, the database serves as a convenient place to find records\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "11\n"
			}
		]
	},
	"12": {
		"page_id": 12,
		"full_text": {
			"audio_url": "",
			"full_text": "(Ex) Record Data\n12\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "(Ex) Record Data\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "12\n"
			}
		]
	},
	"13": {
		"page_id": 13,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ The data is represented as one or more graphs\n▪ (Case 1) Data with relationships among objects\n– The graph captures relationships among data objects \n• Nodes: data objects\n• Links: the relationships among objects\n– (ex) World Wide Web, social networks\n▪ (Case 2) Data with objects that are graphs\n– Each data object is represented as a graph\n– (ex) chemical compounds\n(2) Graph-Based Data\n13\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "(2) Graph-Based Data\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ The data is represented as one or more graphs\n▪ (Case 1) Data with relationships among objects\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– The graph captures relationships among data objects \n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• Nodes: data objects\n• Links: the relationships among objects\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– (ex) World Wide Web, social networks\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ (Case 2) Data with objects that are graphs\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Each data object is represented as a graph\n– (ex) chemical compounds\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "13\n"
			}
		]
	},
	"14": {
		"page_id": 14,
		"full_text": {
			"audio_url": "",
			"full_text": "(Ex) Graph-Based Data\n14\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "(Ex) Graph-Based Data\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "14\n"
			}
		]
	},
	"15": {
		"page_id": 15,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ The attribute values have order relationships in time or space\n▪ (Case 1) Sequential transaction data\n– Each transaction has a timestamp associated with it\n– It is possible to find sequential patterns\n• (ex) people who buy DVD players tend to buy DVDs\n– (ex) retail transaction data, purchase history\n▪ (Case 2) Time series data\n– Each record is a time series (i.e., a series of measurements taken over time)\n– It is important to consider temporal autocorrelation\n• i.e., two values close in time are often very similar\n– (ex) the daily prices of stocks, temperature data\n(3) Ordered Data (1/2)\n15\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "(3) Ordered Data (1/2)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ The attribute values have order relationships in time or space\n▪ (Case 1) Sequential transaction data\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Each transaction has a timestamp associated with it\n– It is possible to find sequential patterns\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• (ex) people who buy DVD players tend to buy DVDs\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– (ex) retail transaction data, purchase history\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ (Case 2) Time series data\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Each record is a time series (i.e., a series of measurements taken over time)\n– It is important to consider temporal autocorrelation\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• i.e., two values close in time are often very similar\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– (ex) the daily prices of stocks, temperature data\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "15\n"
			}
		]
	},
	"16": {
		"page_id": 16,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ (Case 3) Sequence data\n– A data set is a sequence of individual entities\n– There are no time stamps\n• Instead, there are positions in a sequence\n– Many problems involve finding similar sequences\n– (ex) sequences of words, genetic sequence data\n▪ (Case 4) Spatial and spatio-temporal data\n– The data consists of time series at various locations\n– A more complete analysis requires consideration of \nboth the spatial and temporal aspects of the data\n– It is important to consider spatial autocorrelation\n– (ex) Earth science data sets, gas flow simulation data\n(3) Ordered Data (2/2)\n16\ntime\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "(3) Ordered Data (2/2)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ (Case 3) Sequence data\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– A data set is a sequence of individual entities\n– There are no time stamps\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• Instead, there are positions in a sequence\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Many problems involve finding similar sequences\n– (ex) sequences of words, genetic sequence data\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ (Case 4) Spatial and spatio-temporal data\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– The data consists of time series at various locations\n– A more complete analysis requires consideration of \nboth the spatial and temporal aspects of the data\n– It is important to consider spatial autocorrelation\n– (ex) Earth science data sets, gas flow simulation data\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "time\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "16\n"
			}
		]
	},
	"17": {
		"page_id": 17,
		"full_text": {
			"audio_url": "",
			"full_text": "(Ex) Ordered Data\n17\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "(Ex) Ordered Data\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "17\n"
			}
		]
	},
	"18": {
		"page_id": 18,
		"full_text": {
			"audio_url": "",
			"full_text": "Data Quality\n18\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 40,
				"text": "Data Quality\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "18\n"
			}
		]
	},
	"19": {
		"page_id": 19,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ It is unrealistic to expect that data will be perfect\n– Human error\n– Limitations of measuring devices\n– Flaws in the data collection process, etc.\n▪ Examples: data quality problems\n– Values or even entire data objects can be missing\n– Spurious or duplicate objects (e.g., multiple records for a single person)\n– Inconsistencies (e.g., a person has a height of 2 m, but weights only 2 kg)\n▪ To prevent data quality problems, data mining focuses on\n① The detection and correction of data quality problems → data cleaning\n② The use of algorithms that can tolerate poor data quality\nData Quality\n19\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Data Quality\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ It is unrealistic to expect that data will be perfect\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Human error\n– Limitations of measuring devices\n– Flaws in the data collection process, etc.\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Examples: data quality problems\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Values or even entire data objects can be missing\n– Spurious or duplicate objects (e.g., multiple records for a single person)\n– Inconsistencies (e.g., a person has a height of 2 m, but weights only 2 kg)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ To prevent data quality problems, data mining focuses on\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "① The detection and correction of data quality problems → data cleaning\n② The use of algorithms that can tolerate poor data quality\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "19\n"
			}
		]
	},
	"20": {
		"page_id": 20,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Measurement error\n– Any problem resulting from the measurement process\n• (ex) the numerical difference of the measured and true value (i.e., error)\n▪ Data collection error\n– Errors such as omitting data objects or attribute values, or \ninappropriately including a data object\n• (ex) including similar but unrelated data objects\nMeasurement and Data Collection Errors\n20\n?\n?\n?\n?\n?\n?\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Measurement and Data Collection Errors\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Measurement error\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Any problem resulting from the measurement process\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• (ex) the numerical difference of the measured and true value (i.e., error)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Data collection error\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Errors such as omitting data objects or attribute values, or \ninappropriately including a data object\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• (ex) including similar but unrelated data objects\n?\n?\n?\n?\n?\n?\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "20\n"
			}
		]
	},
	"21": {
		"page_id": 21,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Noise\n– The random component of a measurement error\n• Typically involves the distortion of a value or the addition of spurious values\n– Because its elimination is difficult, much work focuses on robust algorithms\n• They produce acceptable results even when noise is present\n▪ Artifacts\n– Deterministic distortions of data\n• (ex) a streak in the same place on a set of photographs\nNoise and Artifacts\n21\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Noise and Artifacts\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Noise\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– The random component of a measurement error\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• Typically involves the distortion of a value or the addition of spurious values\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Because its elimination is difficult, much work focuses on robust algorithms\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• They produce acceptable results even when noise is present\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Artifacts\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Deterministic distortions of data\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• (ex) a streak in the same place on a set of photographs\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "21\n"
			}
		]
	},
	"22": {
		"page_id": 22,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Data objects that have characteristics that are different from \nmost of the other data objects in the data set\n▪ Or, values that are unusual with respect to the typical values\n▪ Also referred to as anomalous objects or values\n▪ Many different definitions have been proposed by the \nstatisticians and data mining communities\n▪ It is important to distinguish between noise and outliers\n– Outliers can be legitimate data objects or values\nOutliers\n22\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Outliers\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Data objects that have characteristics that are different from \nmost of the other data objects in the data set\n▪ Or, values that are unusual with respect to the typical values\n▪ Also referred to as anomalous objects or values\n▪ Many different definitions have been proposed by the \nstatisticians and data mining communities\n▪ It is important to distinguish between noise and outliers\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Outliers can be legitimate data objects or values\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "22\n"
			}
		]
	},
	"23": {
		"page_id": 23,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ The information was not collected or not applicable\n▪ Several strategies for dealing with missing data\n– Eliminate data objects or attributes\n– Estimate missing values (e.g., average, interpolation)\n– Ignore the missing value during analysis\nMissing Values\n23\nName\nHeight\nWeight\nLee\n172 cm\n65 kg\nKim\nX\n74 kg\nPark\n182 cm\n75 kg\nSeo\n169 cm\n62 kg\nJeong\n178 cm\n69 kg\n181 cm\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Missing Values\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ The information was not collected or not applicable\n▪ Several strategies for dealing with missing data\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Eliminate data objects or attributes\n– Estimate missing values (e.g., average, interpolation)\n– Ignore the missing value during analysis\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "181 cm\nName\nLee\nKim\nPark\nSeo\nJeong\nHeight\n172 cm\nX\n182 cm\n169 cm\n178 cm\nWeight\n65 kg\n74 kg\n75 kg\n62 kg\n69 kg\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "23\n"
			}
		]
	},
	"24": {
		"page_id": 24,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Values that violate given consistency constraints\n▪ Examples\n– Different zip codes for the same area\n– A person’s height is negative\n– Nonexistent name\n– 6-digit telephone number \n▪ It is important to detect and, if possible, correct such problems\n– The correction may require additional or external information\nInconsistent Values\n24\nName\nCity\nTel\nLee\nSeoul\n031-710-4112\nKim\nDaejeon\n042-270-4615\nPark\nBusan\n051-200-1679\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Inconsistent Values\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Values that violate given consistency constraints\n▪ Examples\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Different zip codes for the same area\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "Name\nCity\nTel\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– A person’s height is negative\n– Nonexistent name\n– 6-digit telephone number \n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "Lee\nKim\nPark\nSeoul\n031-710-4112\nDaejeon 042-270-4615\nBusan\n051-200-1679\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ It is important to detect and, if possible, correct such problems\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– The correction may require additional or external information\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "24\n"
			}
		]
	},
	"25": {
		"page_id": 25,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Data objects that are duplicates of one another\n▪ Two main issues\n① If there are two objects that actually represent a single object, then it is \nimportant to resolve inconsistent values\n② Care needs to be taken to avoid accidentally combining data objects that\nare similar, but not duplicates (e.g., two people with identical names)\n▪ Deduplication\n– The process of dealing with these issues\nDuplicate Data\n25\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Duplicate Data\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Data objects that are duplicates of one another\n▪ Two main issues\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "① If there are two objects that actually represent a single object, then it is \nimportant to resolve inconsistent values\n② Care needs to be taken to avoid accidentally combining data objects that\nare similar, but not duplicates (e.g., two people with identical names)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Deduplication\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– The process of dealing with these issues\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "25\n"
			}
		]
	},
	"26": {
		"page_id": 26,
		"full_text": {
			"audio_url": "",
			"full_text": "Data Preprocessing\n26\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 40,
				"text": "Data Preprocessing\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "26\n"
			}
		]
	},
	"27": {
		"page_id": 27,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Additional steps to make the data more suitable for data mining\n▪ A broad area and consists of a number of different strategies \nand techniques that are interrelated in complex ways\n▪ We will discuss the following topics:\n– Aggregation\n– Sampling\n– Dimensionality reduction\n– Feature selection\n– Feature creation\n– Discretization and binarization\n– Variable transformation\nData Preprocessing\n27\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Data Preprocessing\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Additional steps to make the data more suitable for data mining\n▪ A broad area and consists of a number of different strategies \nand techniques that are interrelated in complex ways\n▪ We will discuss the following topics:\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Aggregation\n– Sampling\n– Dimensionality reduction\n– Feature selection\n– Feature creation\n– Discretization and binarization\n– Variable transformation\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "27\n"
			}
		]
	},
	"28": {
		"page_id": 28,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Combine two or more objects into a single object\n– Because sometimes “less is more”\n▪ Example: customer purchase data set\n① Replace all the transactions of a single store location with a single object\n② Reduce the possible values for Date from 365 days to 12 months\n1. Aggregation\nItems\nStore Location\nTotal Price\n…\n:\n:\n:\n:\nWatch, Battery, …\nChicago\n$428.98\n…\nShoes, …\nMinneapolis\n$195.02\n…\nItems\nDate\nTotal Price\n…\n:\n:\n:\n:\nWatch, Battery, Shoes, …\n09/06\n$1523.75\n…\n①\n②\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "1. Aggregation\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Combine two or more objects into a single object\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Because sometimes “less is more”\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Example: customer purchase data set\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "① Replace all the transactions of a single store location with a single object\n② Reduce the possible values for Date from 365 days to 12 months\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "①\n②\n"
			},
			{
				"audio_url": "",
				"font_size": 12,
				"text": "…\n"
			}
		]
	},
	"29": {
		"page_id": 29,
		"full_text": {
			"audio_url": "",
			"full_text": "1. The smaller data sets require less memory and processing time\n– Hence, it enables the use of more expensive data mining algorithms\n2. Aggregation can provide a high-level view of the data\n– (ex) each store’s sales → each location’s sales\n3. The behavior of groups of objects is often more stable than \nthat of individual objects\n– (ex) hourly temperature → daily temperature (on average)\n▪ Disadvantage: the potential loss of interesting details\n– (ex) aggregating over months → which day has the highest sales?\nMotivations for Aggregation\n29\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Motivations for Aggregation\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "1. The smaller data sets require less memory and processing time\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Hence, it enables the use of more expensive data mining algorithms\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "2. Aggregation can provide a high-level view of the data\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– (ex) each store’s sales → each location’s sales\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "3. The behavior of groups of objects is often more stable than \nthat of individual objects\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– (ex) hourly temperature → daily temperature (on average)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Disadvantage: the potential loss of interesting details\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– (ex) aggregating over months → which day has the highest sales?\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "29\n"
			}
		]
	},
	"30": {
		"page_id": 30,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Select a subset of the data objects to be analyzed\n▪ Motivations for sampling\n– Statisticians: obtaining the entire data set is too expensive\n– Data miner: processing the entire data set is too expensive\n• In terms of memory or processing time\n▪ Key principle for effective sampling\n– Use a representative sample\n• It should have approximately the same property as the original data set\n• (ex) the mean of a sample  the mean of the original data set\n2. Sampling\n30\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "2. Sampling\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Select a subset of the data objects to be analyzed\n▪ Motivations for sampling\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Statisticians: obtaining the entire data set is too expensive\n– Data miner: processing the entire data set is too expensive\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• In terms of memory or processing time\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Key principle for effective sampling\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Use a representative sample\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• It should have approximately the same property as the original data set\n• (ex) the mean of a sample  the mean of the original data set\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "30\n"
			}
		]
	},
	"31": {
		"page_id": 31,
		"full_text": {
			"audio_url": "",
			"full_text": "① Simple random sampling\n– There is an equal probability of selecting any particular object\n– Two variations on random sampling\n• Sampling without replacement\n• Sampling with replacement\n② Stratified sampling\n– When the population consists of different types of objects, simple \nrandom sampling can fail\n• (ex) A: 10000, B: 10 → A: 100, B: 0\n– Select objects from each group\n• Equal numbers of objects\n• The number proportional to the size of that group\nSampling Approaches\n31\nwithout\nreplacement\nwith\nreplacement\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Sampling Approaches\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "① Simple random sampling\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– There is an equal probability of selecting any particular object\n– Two variations on random sampling\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• Sampling without replacement\n• Sampling with replacement\n"
			},
			{
				"audio_url": "",
				"font_size": 16,
				"text": "without\nreplacement\nwith\nreplacement\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "② Stratified sampling\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– When the population consists of different types of objects, simple \nrandom sampling can fail\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• (ex) A: 10000, B: 10 → A: 100, B: 0\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Select objects from each group\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• Equal numbers of objects\n• The number proportional to the size of that group\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "31\n"
			}
		]
	},
	"32": {
		"page_id": 32,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Used when the proper sample size can be difficult to determine\n▪ Basic technique\n– Starts with a small sample\n– Increase the sample size until a sample of sufficient size has been obtained\n▪ Important point\n– There must be a way to evaluate the sample to judge if it is large enough\n• (ex) Stop increasing the sample size if the increase in accuracy levels off\nProgressive (or Adaptive) Sampling\n32\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Progressive (or Adaptive) Sampling\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Used when the proper sample size can be difficult to determine\n▪ Basic technique\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Starts with a small sample\n– Increase the sample size until a sample of sufficient size has been obtained\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Important point\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– There must be a way to evaluate the sample to judge if it is large enough\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• (ex) Stop increasing the sample size if the increase in accuracy levels off\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "32\n"
			}
		]
	},
	"33": {
		"page_id": 33,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ The process of reducing the number of attributes in the data set\n– Dimensionality = the number of attributes\n▪ Key benefits\n① Many data mining algorithms work better if the dimensionality is lower\n• Partly because irrelevant features are eliminated and noise is reduced\n• Partly because the curse of dimensionality\n② A more understandable model can be obtained\n• Because the model involves fewer attributes\n• (ex) y = x1 + 5.1x2 + 4.2x3 + 8.7x4 + 7.4x5 + 2.9x6 + 10x7 → y = z1 + 7.2z2\n3. Dimensionality Reduction\n33\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "3. Dimensionality Reduction\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ The process of reducing the number of attributes in the data set\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Dimensionality = the number of attributes\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Key benefits\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "① Many data mining algorithms work better if the dimensionality is lower\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• Partly because irrelevant features are eliminated and noise is reduced\n• Partly because the curse of dimensionality\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "② A more understandable model can be obtained\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• Because the model involves fewer attributes\n"
			},
			{
				"audio_url": "",
				"font_size": 12,
				"text": "• (ex) y = x1 + 5.1x2 + 4.2x3 + 8.7x4 + 7.4x5 + 2.9x6 + 10x7 → y = z1 + 7.2z2\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "33\n"
			}
		]
	},
	"34": {
		"page_id": 34,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Key benefits (cont’d)\n③ The data can be more easily visualized \n• Because the data can be reduced to two or three dimensions\n④ The amount of time and memory required by the algorithm is reduced\n• Because the size of the data is reduced\n3. Dimensionality Reduction\n34\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "3. Dimensionality Reduction\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Key benefits (cont’d)\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "③ The data can be more easily visualized \n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• Because the data can be reduced to two or three dimensions\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "④ The amount of time and memory required by the algorithm is reduced\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• Because the size of the data is reduced\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "34\n"
			}
		]
	},
	"35": {
		"page_id": 35,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ The phenomenon that data analysis becomes significantly harder \nas the dimensionality of the data increases\n▪ Because, as dimensionality increases, the data becomes \nincreasingly sparse in the space\n– Also the distances between objects become very large\n– Eventually the distances between objects become almost the same\nThe Curse of Dimensionality (1/2)\n35\n1D (50% of data)\n2D (25% of data)\n3D (12.5% of data)\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "The Curse of Dimensionality (1/2)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ The phenomenon that data analysis becomes significantly harder \nas the dimensionality of the data increases\n▪ Because, as dimensionality increases, the data becomes \nincreasingly sparse in the space\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Also the distances between objects become very large\n– Eventually the distances between objects become almost the same\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "1D (50% of data)\n2D (25% of data)\n3D (12.5% of data)\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "35\n"
			}
		]
	},
	"36": {
		"page_id": 36,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Problem for classification\n– There are not enough data objects to allow the creation of a model that \nreliably assigns a class to all possible objects\n▪ Problem for clustering\n– The distance between objects become less meaningful\nThe Curse of Dimensionality (2/2)\n36\n1. The amount of training \ndata needed to cover \n80% of the space \ngrows exponentially\n2. Nearly all objects \nbecome far from each \nother\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "The Curse of Dimensionality (2/2)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Problem for classification\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– There are not enough data objects to allow the creation of a model that \nreliably assigns a class to all possible objects\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Problem for clustering\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– The distance between objects become less meaningful\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "1. The amount of training \ndata needed to cover \n80% of the space \ngrows exponentially\n2. Nearly all objects \nbecome far from each \nother\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "36\n"
			}
		]
	},
	"37": {
		"page_id": 37,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Another way to reduce the dimensionality is to use only a subset\nof the features\n– We would not lose information if redundant and irrelevant features are \npresent\n▪ Redundant features\n– Duplicate much or all of the information contained in other attributes\n– (ex) the price of a product  the amount of sales tax\n▪ Irrelevant features\n– Contain almost no useful information for the data mining task\n– (ex) ‘student ID’ for the task of predicting students’ GPA\n4. Feature Selection\n37\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "4. Feature Selection\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Another way to reduce the dimensionality is to use only a subset\nof the features\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– We would not lose information if redundant and irrelevant features are \npresent\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Redundant features\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Duplicate much or all of the information contained in other attributes\n– (ex) the price of a product  the amount of sales tax\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Irrelevant features\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Contain almost no useful information for the data mining task\n– (ex) ‘student ID’ for the task of predicting students’ GPA\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "37\n"
			}
		]
	},
	"38": {
		"page_id": 38,
		"full_text": {
			"audio_url": "",
			"full_text": "1. Use common sense or domain knowledge\n2. Embedded approaches\n– The data mining algorithm itself decides which attributes to use\n– (ex) decision trees\n3. Filter approaches\n– Features are selected before the data mining algorithm is run\n– (ex) select attributes whose pairwise correlation is as low as possible\n4. Wrapper approaches\n– Use the target data mining algorithm as a black box to find the best\nsubset of attributes\n– (ex) add attributes one by one as long as the performance improves\nApproaches to Feature Selection\n38\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Approaches to Feature Selection\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "1. Use common sense or domain knowledge\n2. Embedded approaches\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– The data mining algorithm itself decides which attributes to use\n– (ex) decision trees\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "3. Filter approaches\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Features are selected before the data mining algorithm is run\n– (ex) select attributes whose pairwise correlation is as low as possible\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "4. Wrapper approaches\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Use the target data mining algorithm as a black box to find the best\nsubset of attributes\n– (ex) add attributes one by one as long as the performance improves\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "38\n"
			}
		]
	},
	"39": {
		"page_id": 39,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ An alternative to keeping or eliminating features\n– Assign more important features a higher weight, while giving less \nimportant features a lower weight\n▪ Two approaches\n– Use domain knowledge about the relative importance of features\n– The data mining algorithm determines the weights automatically\n▪ (ex) support vector machine (SVM)\n– Produces classification models in which each feature is given a weight\n– (ex) y = 100x1 + 0.01x2 + 20x3 + 4\n• x1 is the most important feature, while x2 is the least important feature\nFeature Weighting\n39\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Feature Weighting\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ An alternative to keeping or eliminating features\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Assign more important features a higher weight, while giving less \nimportant features a lower weight\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Two approaches\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Use domain knowledge about the relative importance of features\n– The data mining algorithm determines the weights automatically\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ (ex) support vector machine (SVM)\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Produces classification models in which each feature is given a weight\n– (ex) y = 100x1 + 0.01x2 + 20x3 + 4\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• x1 is the most important feature, while x2 is the least important feature\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "39\n"
			}
		]
	},
	"40": {
		"page_id": 40,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ It is frequently possible to create, from the original attributes, \nnew attributes\n– That captures the important information in a data set much more \neffectively\n▪ Two related methodologies\n① Feature extraction\n② Mapping the data to a new space\n5. Feature Creation\n40\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "5. Feature Creation\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ It is frequently possible to create, from the original attributes, \nnew attributes\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– That captures the important information in a data set much more \neffectively\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Two related methodologies\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "① Feature extraction\n② Mapping the data to a new space\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "40\n"
			}
		]
	},
	"41": {
		"page_id": 41,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ The creation of a new set of features from the original raw data\n▪ Example\n– We want to classify historical artifacts with respect to their materials\n• (ex) wood, clay, bronze, gold\n– In this case, a density feature constructed from the mass and volume \nfeatures would most directly yield an accurate classification \n▪ Unfortunately, the most common approach is to use domain \nexpertise\nFeature Extraction\n41\nArtifact\nMass\nVolume\nArtifact\nMass\nVolume\nDensity (Mass/Volume)\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Feature Extraction\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ The creation of a new set of features from the original raw data\n▪ Example\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– We want to classify historical artifacts with respect to their materials\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• (ex) wood, clay, bronze, gold\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– In this case, a density feature constructed from the mass and volume \nfeatures would most directly yield an accurate classification \n"
			},
			{
				"audio_url": "",
				"font_size": 16,
				"text": "Artifact\nMass\nVolume\nArtifact\nMass\nVolume Density (Mass/Volume)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Unfortunately, the most common approach is to use domain \nexpertise\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "41\n"
			}
		]
	},
	"42": {
		"page_id": 42,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ A totally different view of the data can reveal important and \ninteresting features\n▪ Example\n– The following points represented in the Euclidean space (x, y) are difficult \nfor decision trees to classify\n– However, if we represent the points in the polar coordinate system (r, θ), \nit is easy for decision trees to classify the points\nMapping the Data to a New Space\n42\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Mapping the Data to a New Space\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ A totally different view of the data can reveal important and \ninteresting features\n▪ Example\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– The following points represented in the Euclidean space (x, y) are difficult \nfor decision trees to classify\n– However, if we represent the points in the polar coordinate system (r, θ), \nit is easy for decision trees to classify the points\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "42\n"
			}
		]
	},
	"43": {
		"page_id": 43,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Discretization\n– Transform a continuous attribute into a categorical attribute\n▪ Binarization \n– Transform an attribute into one or more binary attributes\n▪ Why?\n– Some data mining algorithms require categorical or binary attributes\n• (ex) certain classification algorithms, association rule mining algorithms\n6. Discretization and Binarization\n43\nHumidity\n85.1\n78.2\n62.6\nHumidity\nHigh\nNormal\nLow\nName\nGender\nAge\nLee\nMale\n24\nKim\nFemale\n17\nName\nMale\nFemale\nAge\nLee\n1\n0\n24\nKim\n0\n1\n17\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "6. Discretization and Binarization\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Discretization\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Transform a continuous attribute into a categorical attribute\n"
			},
			{
				"audio_url": "",
				"font_size": 16,
				"text": "Humidity\n85.1\n78.2\n62.6\nHumidity\nHigh\nNormal\nLow\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Binarization \n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Transform an attribute into one or more binary attributes\n"
			},
			{
				"audio_url": "",
				"font_size": 16,
				"text": "Name Gender\nAge\nLee\nKim\nMale\nFemale\n24\n17\nName Male\nFemale\nAge\nLee\nKim\n1\n0\n0\n1\n24\n17\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Why?\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Some data mining algorithms require categorical or binary attributes\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• (ex) certain classification algorithms, association rule mining algorithms\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "43\n"
			}
		]
	},
	"44": {
		"page_id": 44,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Simple technique\n– Suppose there are m categorical values\n– Introduce one binary attribute for each categorical value\n– For each of the m binary attributes\n• Assign 1, if the binary attribute represents the categorical value of the object\n• Assign 0, otherwise\nBinarization\n44\nawful     poor        OK       good     great\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Binarization\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Simple technique\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Suppose there are m categorical values\n– Introduce one binary attribute for each categorical value\n– For each of the m binary attributes\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• Assign 1, if the binary attribute represents the categorical value of the object\n• Assign 0, otherwise\n"
			},
			{
				"audio_url": "",
				"font_size": 16,
				"text": "awful     poor        OK       good     great\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "44\n"
			}
		]
	},
	"45": {
		"page_id": 45,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Basic steps\n– Decide how many categories, n, to have\n– Divide the values of the continuous attribute into n intervals\n– Map all the values in one interval to the same categorical value\n▪ Several simple approaches\n– Equal width discretization\n– Equal frequency discretization\n– Clustering-based discretization (e.g., k-means)\nDiscretization\n45\nx0\nx1\nx2\nx3\nx4\n…\nxn\nxn-1\nv1\nv2\nv3\nv4\nvn\nvn-1\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Discretization\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Basic steps\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Decide how many categories, n, to have\n– Divide the values of the continuous attribute into n intervals\n– Map all the values in one interval to the same categorical value\n"
			},
			{
				"audio_url": "",
				"font_size": 12,
				"text": "x0\nx1\nx2\nx3\nx4\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "…\n"
			},
			{
				"audio_url": "",
				"font_size": 12,
				"text": "xn-1\nxn\nv1\nv2\nv3\nv4\nvn-1\nvn\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Several simple approaches\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Equal width discretization\n– Equal frequency discretization\n– Clustering-based discretization (e.g., k-means)\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "45\n"
			}
		]
	},
	"46": {
		"page_id": 46,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Equal width discretization\n– Divide the range into a number of intervals each having the same width\n▪ Equal frequency discretization\n– Try to put the same number of objects into each interval\n▪ Clustering-based discretization (e.g., k-means)\n– Find clusters of objects and divide the range according to the clusters\nSeveral Approaches to Discretization\n46\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Several Approaches to Discretization\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Equal width discretization\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Divide the range into a number of intervals each having the same width\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Equal frequency discretization\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Try to put the same number of objects into each interval\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Clustering-based discretization (e.g., k-means)\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Find clusters of objects and divide the range according to the clusters\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "46\n"
			}
		]
	},
	"47": {
		"page_id": 47,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Apply a transformation to all the values of a variable (attribute)\n▪ (Type 1) Simple functions\n– Apply a simple mathematical function to each value individually\n• (ex) xk, log x, ex, 𝑥, 1/x, sin x, or |x|\n– Examples\n• log10 x is used when the range of values is very huge (e.g., 108, 109 → 8, 9)\n•\n𝑥, log x, and 1/x are often used to transform data into a normal distribution\n7. Variable Transformation (1/2)\n47\nx\n𝑥\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "7. Variable Transformation (1/2)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Apply a transformation to all the values of a variable (attribute)\n▪ (Type 1) Simple functions\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Apply a simple mathematical function to each value individually\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• (ex) xk, log x, ex,  𝑥, 1/x, sin x, or |x|\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Examples\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• log10 x is used when the range of values is very huge (e.g., 108, 109 → 8, 9)\n•\n𝑥, log x, and 1/x are often used to transform data into a normal distribution\nx\n𝑥\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "47\n"
			}
		]
	},
	"48": {
		"page_id": 48,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ (Type 2) Normalization or standardization\n– Make an entire set of values have a particular property\n– (ex) “Standardizing a variable” in statistics 𝑥′ = (𝑥 − ҧ𝑥)/𝑠𝑥\n•\nҧ𝑥: the mean of the attribute values\n• 𝑠𝑥: the standard deviation of the attribute values\n• Creates a new variable that has a mean of 0 and a standard deviation of 1\n– Often used to avoid having a variable with large values dominate the \nresults of analysis\n• (ex) Consider comparing people based on age and income\n• The comparison between people will be dominated by differences in income\n– (ex) person1 = (24, 25000),  person2 = (67, 25050),  person3 = (25, 25100)\n7. Variable Transformation (2/2)\n48\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "7. Variable Transformation (2/2)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ (Type 2) Normalization or standardization\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Make an entire set of values have a particular property\n"
			},
			{
				"audio_url": "",
				"font_size": 15,
				"text": "– (ex) “Standardizing a variable” in statistics 𝑥′ = (𝑥 − ҧ𝑥)/𝑠𝑥\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "•\nҧ𝑥: the mean of the attribute values\n• 𝑠𝑥: the standard deviation of the attribute values\n• Creates a new variable that has a mean of 0 and a standard deviation of 1\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Often used to avoid having a variable with large values dominate the \nresults of analysis\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• (ex) Consider comparing people based on age and income\n• The comparison between people will be dominated by differences in income\n"
			},
			{
				"audio_url": "",
				"font_size": 16,
				"text": "– (ex) person1 = (24, 25000),  person2 = (67, 25050),  person3 = (25, 25100)\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "48\n"
			}
		]
	},
	"49": {
		"page_id": 49,
		"full_text": {
			"audio_url": "",
			"full_text": "Measures of Similarity\n49\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 40,
				"text": "Measures of Similarity\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "49\n"
			}
		]
	},
	"50": {
		"page_id": 50,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Similarity and dissimilarity between objects are important\n– Because they are used by a number of data mining techniques\n– (ex) clustering, nearest neighbor classification, and anomaly detection\n▪ Proximity\n– Used to refer to either similarity or dissimilarity\n– There are many proximity measures for objects\n• Euclidean distance\n• Jaccard coefficient\n• Cosine similarity\n• …\n▪ We will discuss various proximity measures for objects\nMeasures of Similarity and Dissimilarity\n50\n\nd\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Measures of Similarity and Dissimilarity\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Similarity and dissimilarity between objects are important\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Because they are used by a number of data mining techniques\n– (ex) clustering, nearest neighbor classification, and anomaly detection\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Proximity\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Used to refer to either similarity or dissimilarity\n– There are many proximity measures for objects\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• Euclidean distance\n• Jaccard coefficient\n• Cosine similarity\n• …\nd\n\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ We will discuss various proximity measures for objects\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "50\n"
			}
		]
	},
	"51": {
		"page_id": 51,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Similarity: a numerical measure of the degree to which two \nobjects are alike\n– The more alike, the higher the similarity is\n• (ex) 0 (no similarity) → 1 (complete similarity)\n▪ Dissimilarity: a numerical measure of the degree to which two \nobjects are different\n– The more similar, the lower the dissimilarity is\n• (ex) 0 (complete similarity) → 1 or  (no similarity)\nDefinitions\n51\nSimilarity = 1\nDissimilarity = 0\nSimilarity = 0\nDissimilarity = 1\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Definitions\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Similarity: a numerical measure of the degree to which two \nobjects are alike\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– The more alike, the higher the similarity is\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• (ex) 0 (no similarity) → 1 (complete similarity)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Dissimilarity: a numerical measure of the degree to which two \nobjects are different\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– The more similar, the lower the dissimilarity is\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• (ex) 0 (complete similarity) → 1 or  (no similarity)\nSimilarity = 1\nDissimilarity = 0\nSimilarity = 0\nDissimilarity = 1\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "51\n"
			}
		]
	},
	"52": {
		"page_id": 52,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ A similarity can be converted to a dissimilarity, or vice versa\n▪ Examples\n– Let s ∈ [0, 1] and d ∈ [0, 1] be a similarity and a dissimilarity, respectively\n– s can be converted to d as follows:\n– Subtract: d = 1 – s\n• (ex) s = 0 → d = 1, s = 1 → d = 0\n– Reciprocal: d = 2/(s + 1) – 1\n• (ex) s = 0 → d = 1, s = 1 → d = 0\n– Exponent: d = e–s\n• (ex) s = 0 → d = 1, s = 1 → d = 0.37\n▪ In general, any monotonic decreasing function can be used\nTransformations\n52\ns\nd\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Transformations\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ A similarity can be converted to a dissimilarity, or vice versa\n▪ Examples\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Let s ∈ [0, 1] and d ∈ [0, 1] be a similarity and a dissimilarity, respectively\n– s can be converted to d as follows:\n– Subtract: d = 1 – s\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• (ex) s = 0 → d = 1, s = 1 → d = 0\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Reciprocal: d = 2/(s + 1) – 1\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• (ex) s = 0 → d = 1, s = 1 → d = 0\n"
			},
			{
				"audio_url": "",
				"font_size": 13,
				"text": "– Exponent: d = e–s\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• (ex) s = 0 → d = 1, s = 1 → d = 0.37\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "d\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ In general, any monotonic decreasing function can be used\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "s\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "52\n"
			}
		]
	},
	"53": {
		"page_id": 53,
		"full_text": {
			"audio_url": "",
			"full_text": "1. [Dissimilarity] Distances\n– Manhattan distance (L1 distance)\n– Euclidean distance (L2 distance)\n– Supremum distance (L∞ distance)\n2. [Similarity] Similarity coefficients\n– Simple matching coefficient\n– Jaccard coefficient\n3. [Similarity] Cosine similarity\n4. [Similarity] Correlation\n5. [Similarity] Mutual information\nExamples of Proximity Measures\n53\nx = (1, 3, 6, 2, 9, 7, 5, 4, 10, 8) \ny = (7, 2, 5, 8, 1, 6, 10, 4, 3, 9)\nproximity?\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Examples of Proximity Measures\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "1.\n[Dissimilarity] Distances\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Manhattan distance (L1 distance)\n– Euclidean distance (L2 distance)\n– Supremum distance (L∞ distance)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "2.\n[Similarity] Similarity coefficients\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Simple matching coefficient\n– Jaccard coefficient\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "3.\n[Similarity] Cosine similarity\n4.\n[Similarity] Correlation\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "x = (1, 3, 6, 2, 9, 7, 5, 4, 10, 8) \nproximity?\ny = (7, 2, 5, 8, 1, 6, 10, 4, 3, 9)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "5.\n[Similarity] Mutual information\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "53\n"
			}
		]
	},
	"54": {
		"page_id": 54,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Let x = (x1, x2, …, xn) and y = (y1, y2, …, yn) be two data objects\n– n: the number of dimensions\n– xk and yk: the kth attributes of x and y, respectively\n▪ Distances: dissimilarities with certain properties\n▪ Euclidean distance\n– The straight-line distance between two points\n54\n1. Distances (1/2)\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "1. Distances (1/2)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Let x = (x1, x2, …, xn) and y = (y1, y2, …, yn) be two data objects\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– n: the number of dimensions\n– xk and yk: the kth attributes of x and y, respectively\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Distances: dissimilarities with certain properties\n▪ Euclidean distance\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– The straight-line distance between two points\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "54\n"
			}
		]
	},
	"55": {
		"page_id": 55,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Generalization of the Euclidean distance (Minkowski distance)\n– r = 1: Manhattan distance (L1 norm)\n• d(x, y) = |x1 – y1| + |x2 – y2| + … + |xn – yn|\n– r = 2: Euclidean distance (L2 norm)\n– r = ∞: Supremum distance (Lmax or L∞ norm)\n•\n1. Distances (2/2)\n55\n= max\n𝑘 (|𝑥𝑘 − 𝑦𝑘|)\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "1. Distances (2/2)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Generalization of the Euclidean distance (Minkowski distance)\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– r = 1: Manhattan distance (L1 norm)\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• d(x, y) = |x1 – y1| + |x2 – y2| + … + |xn – yn|\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– r = 2: Euclidean distance (L2 norm)\n– r = ∞: Supremum distance (Lmax or L∞ norm)\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "•\n= max\n"
			},
			{
				"audio_url": "",
				"font_size": 13,
				"text": "𝑘\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "(|𝑥𝑘 − 𝑦𝑘|)\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "55\n"
			}
		]
	},
	"56": {
		"page_id": 56,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Consider the following four two-dimensional points\n(Ex) Minkowski Distance\n56\nL1 distance matrix\nL2 distance matrix\nL∞ distance matrix\nL2\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "(Ex) Minkowski Distance\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Consider the following four two-dimensional points\n"
			},
			{
				"audio_url": "",
				"font_size": 11,
				"text": "L2\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "L1 distance matrix\nL2 distance matrix\nL∞ distance matrix\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "56\n"
			}
		]
	},
	"57": {
		"page_id": 57,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ If d(x, y) is a distance, the following properties hold:\n1. Positivity\n– d(x, y) ≥ 0 for all x and y\n– d(x, y) = 0 only if x = y \n2. Symmetry\n– d(x, y) = d(y, x) for all x and y\n3. Triangle inequality\n– d(x, z) ≤ d(x, y) + d(y, z) for all x, y, and z\n▪ These properties are useful because they express our intuition \nabout a distance well\nThe Properties of Distances\n57\nd(x, z) ≤ d(x, y) + d(y, z)\nd(x, y) \nd(y, z) \ny\nx\nz\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "The Properties of Distances\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ If d(x, y) is a distance, the following properties hold:\n1. Positivity\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– d(x, y) ≥ 0 for all x and y\n– d(x, y) = 0 only if x = y \n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "2. Symmetry\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– d(x, y) = d(y, x) for all x and y\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "3. Triangle inequality\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– d(x, z) ≤ d(x, y) + d(y, z) for all x, y, and z\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "y\nd(x, y) \nd(y, z) \nx\nd(x, z) ≤ d(x, y) + d(y, z)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ These properties are useful because they express our intuition \nabout a distance well\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "z\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "57\n"
			}
		]
	},
	"58": {
		"page_id": 58,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Similarity measures between objects that contain only binary \nattributes\n– Typically have values between 0 and 1\n• 0: the objects are not at all similar\n• 1: the objects are completely similar\n▪ Notations\n– Let x = (x1, x2, …, xn) and y = (y1, y2, …, yn) be two objects\n• where xk and yk are binary attributes (k = 1, 2, …, n)\n– f00 = the number of attributes where x is 0 and y is 0\n– f01 = the number of attributes where x is 0 and y is 1\n– f10 = the number of attributes where x is 1 and y is 0\n– f11 = the number of attributes where x is 1 and y is 1\n2. Similarity Coefficients (1/2)\n58\nx = (1, 0, 0, 1, 0, 1, 0, 0, 0, 1) \ny = (0, 1, 0, 1, 1, 0, 0, 0, 1, 0)\nSimilarity?\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "2. Similarity Coefficients (1/2)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Similarity measures between objects that contain only binary \nattributes\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Typically have values between 0 and 1\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• 0: the objects are not at all similar\n• 1: the objects are completely similar\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "x = (1, 0, 0, 1, 0, 1, 0, 0, 0, 1) \nSimilarity?\ny = (0, 1, 0, 1, 1, 0, 0, 0, 1, 0)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Notations\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Let x = (x1, x2, …, xn) and y = (y1, y2, …, yn) be two objects\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• where xk and yk are binary attributes (k = 1, 2, …, n)\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– f00 = the number of attributes where x is 0 and y is 0\n– f01 = the number of attributes where x is 0 and y is 1\n– f10 = the number of attributes where x is 1 and y is 0\n– f11 = the number of attributes where x is 1 and y is 1\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "58\n"
			}
		]
	},
	"59": {
		"page_id": 59,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Simple matching coefficient (SMC)\n– Counts both presences and absences equally\n▪ Jaccard coefficient (J)\n– Counts only presences (e.g., items purchased by both customers)\n▪ Example\n– x = (1, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n– y = (0, 0, 0, 0, 0, 0, 1, 0, 0, 1)\n– SMC = (f11 + f00)/(f01 + f10 + f11 + f00) = 0.7, J = f11 /(f01 + f10 + f11) = 0\n2. Similarity Coefficients (2/2)\n59\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "2. Similarity Coefficients (2/2)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Simple matching coefficient (SMC)\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Counts both presences and absences equally\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Jaccard coefficient (J)\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Counts only presences (e.g., items purchased by both customers)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Example\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– x = (1, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n– y = (0, 0, 0, 0, 0, 0, 1, 0, 0, 1)\n– SMC = (f11 + f00)/(f01 + f10 + f11 + f00) = 0.7, J = f11 /(f01 + f10 + f11) = 0\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "59\n"
			}
		]
	},
	"60": {
		"page_id": 60,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Measure the (cosine of the) angle between two vectors x and y\n– <x, y>: the inner product of x and y, i.e., \n– ||x||: the length of vector x, i.e., \n3. Cosine Similarity (1/2)\n60\n  0 → cos(x, y)  1\nx\nx\ny\ny\n\n\nx\ny\n\n  180 → cos(x, y)  –1\n  90 → cos(x, y)  0\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "3. Cosine Similarity (1/2)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Measure the (cosine of the) angle between two vectors x and y\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– <x, y>: the inner product of x and y, i.e., \n– ||x||: the length of vector x, i.e., \n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "x\ny\n\nx\ny\n\ny\n\nx\n  0 → cos(x, y)  1\n  90 → cos(x, y)  0\n  180 → cos(x, y)  –1\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "60\n"
			}
		]
	},
	"61": {
		"page_id": 61,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Useful for measuring the similarity between documents\n– Documents are often represented as vectors\n• Each component represents the frequency of a particular term (word)\n– 0-0 matches are ignored (i.e., words that do not appear in both)\n• If 0-0 matches are counted, most documents will be similar to each other\n– Depends only upon the words that appear in both documents\n▪ (ex) Cosine similarity between two document vectors\n– x = (3, 2, 0, 5, 0, 0, 0, 0, 2, 0, 0)\n– y = (1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2)\n– cos(x, y) = <x, y>/(||x||||y||) = 5/(6.482.45) = 0.31\n▪ Note that the lengths of x and y are not important in cos(x, y) \n3. Cosine Similarity (2/2)\n61\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "3. Cosine Similarity (2/2)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Useful for measuring the similarity between documents\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Documents are often represented as vectors\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• Each component represents the frequency of a particular term (word)\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– 0-0 matches are ignored (i.e., words that do not appear in both)\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• If 0-0 matches are counted, most documents will be similar to each other\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Depends only upon the words that appear in both documents\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ (ex) Cosine similarity between two document vectors\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– x = (3, 2, 0, 5, 0, 0, 0, 0, 2, 0, 0)\n– y = (1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2)\n– cos(x, y) = <x, y>/(||x||||y||) = 5/(6.482.45) = 0.31\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Note that the lengths of x and y are not important in cos(x, y) \n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "61\n"
			}
		]
	},
	"62": {
		"page_id": 62,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Measure the linear relationship between two sets of values\n– Examples\n• x = (1, 2, 3, 4, 5), y = (2, 4, 6, 8, 10) → perfect positive correlation (= 1)\n• x = (1, 2, 3, 4, 5), y = (5, 4, 3, 2, 1) → perfect negative correlation (= –1)\n▪ There are many types of correlation\n– In this course, we focus on Pearson’s correlation\n4. Correlation\n62\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "4. Correlation\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Measure the linear relationship between two sets of values\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Examples\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• x = (1, 2, 3, 4, 5), y = (2, 4, 6, 8, 10) → perfect positive correlation (= 1)\n• x = (1, 2, 3, 4, 5), y = (5, 4, 3, 2, 1) → perfect negative correlation (= –1)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ There are many types of correlation\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– In this course, we focus on Pearson’s correlation\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "62\n"
			}
		]
	},
	"63": {
		"page_id": 63,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Definition\n– where we use the following standard statistical notation and definitions:\nPearson’s Correlation\n63\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Pearson’s Correlation\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Definition\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– where we use the following standard statistical notation and definitions:\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "63\n"
			}
		]
	},
	"64": {
		"page_id": 64,
		"full_text": {
			"audio_url": "",
			"full_text": "▪\nPerfect negative correlation\n– x = (–3, 6, 0, 3, –6)\n– y = (1, –2, 0, – 1, 2)\n– corr(x, y) = –1 (⸪ xk = –3yk)\n▪\nPerfect positive correlation\n– x = (3, 6, 0, 3, 6)\n– y = (1, 2, 0, 1, 2)\n– corr(x, y) = 1 (⸪ xk = 3yk)\n▪\nNo linear correlation\n– x = (–3, –2, –1, 0, 1, 2, 3)\n– y = (9, 4, 1, 0, 1, 4, 9)\n– corr(x, y) = 0 (⸪ yk = xk\n2)\n(Ex) Pearson’s Correlation\n64\nCorrelations from –1 to 1\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "(Ex) Pearson’s Correlation\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "▪ Perfect negative correlation\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "– x = (–3, 6, 0, 3, –6)\n– y = (1, –2, 0, – 1, 2)\n"
			},
			{
				"audio_url": "",
				"font_size": 16,
				"text": "– corr(x, y) = –1 (⸪ xk = –3yk)\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "▪ Perfect positive correlation\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "– x = (3, 6, 0, 3, 6)\n– y = (1, 2, 0, 1, 2)\n"
			},
			{
				"audio_url": "",
				"font_size": 16,
				"text": "– corr(x, y) = 1 (⸪ xk = 3yk)\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "▪ No linear correlation\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "– x = (–3, –2, –1, 0, 1, 2, 3)\n– y = (9, 4, 1, 0, 1, 4, 9)\n"
			},
			{
				"audio_url": "",
				"font_size": 12,
				"text": "– corr(x, y) = 0 (⸪ yk = xk\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "2)\nCorrelations from –1 to 1\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "64\n"
			}
		]
	},
	"65": {
		"page_id": 65,
		"full_text": {
			"audio_url": "",
			"full_text": "(Ex) Comparing Proximity Measures\n65\nx = (1, 2, 4, 3, 0, 0, 0)\ny = (1, 2, 3, 4, 0, 0, 0)\nx = (1, 2, 4, 3, 0, 0, 0)\nys = (2, 4, 6, 8, 0, 0, 0)\n(ys = 2y)\nx = (1, 2, 4, 3, 0, 0, 0)\nyt = (6, 7, 8, 9, 5, 5, 5)\n(yt = y + 5)\ncos(x, y)\n0.9667\n0.9667\n0.7940\ncorr(x, y)\n0.9429\n0.9429\n0.9429\nEuclidean\ndistance(x, y)\n1.4142\n5.8310\n14.2127\nMeasure\nObjects\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "(Ex) Comparing Proximity Measures\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "Objects\nMeasure\nx = (1, 2, 4, 3, 0, 0, 0)\ny = (1, 2, 3, 4, 0, 0, 0)\nx = (1, 2, 4, 3, 0, 0, 0)\nys = (2, 4, 6, 8, 0, 0, 0)\n(ys = 2y)\nx = (1, 2, 4, 3, 0, 0, 0)\nyt = (6, 7, 8, 9, 5, 5, 5)\n(yt = y + 5)\ncos(x, y)\n0.9667\n0.9667\n0.7940\ncorr(x, y)\n0.9429\n0.9429\n0.9429\nEuclidean\ndistance(x, y)\n1.4142\n5.8310\n14.2127\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "65\n"
			}
		]
	},
	"66": {
		"page_id": 66,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Measure the similarity between two sets of paired values\n– Particularly when a nonlinear relationship is suspected\n▪ Measure how much information one set of values provides \nabout another \n– Given that the values in pairs (e.g., height and weight)\n▪ Intuitive example (0: head, 1: tail)\n66\nMutual Information (1/2)\nx\ny\nMutual information\n(0, 0, 0, 0, 0, 1, 1, 1, 1, 1)\n(0, 0, 0, 0, 0, 1, 1, 1, 1, 1)\n1\n(0, 0, 0, 0, 0, 1, 1, 1, 1, 1)\n(1, 1, 1, 1, 1, 0, 0, 0, 0, 0)\n1\n(0, 0, 0, 0, 0, 1, 1, 1, 1, 1)\n(0, 0, 1, 0, 0, 0, 1, 0, 1, 1)\n0.1535\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Mutual Information (1/2)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Measure the similarity between two sets of paired values\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Particularly when a nonlinear relationship is suspected\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Measure how much information one set of values provides \nabout another \n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Given that the values in pairs (e.g., height and weight)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Intuitive example (0: head, 1: tail)\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "x\ny\nMutual information\n(0, 0, 0, 0, 0, 1, 1, 1, 1, 1)\n(0, 0, 0, 0, 0, 1, 1, 1, 1, 1)\n(0, 0, 0, 0, 0, 1, 1, 1, 1, 1)\n(1, 1, 1, 1, 1, 0, 0, 0, 0, 0)\n1\n1\n(0, 0, 0, 0, 0, 1, 1, 1, 1, 1)\n(0, 0, 1, 0, 0, 0, 1, 0, 1, 1)\n0.1535\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "66\n"
			}
		]
	},
	"67": {
		"page_id": 67,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ If the two sets of values are completely independent\n– i.e., the value of one tells us nothing about the other\n– Then their mutual information is 0\n▪ If the two sets of values are completely dependent\n– i.e., knowing the value of one tells us the value of the other\n– Then they have maximum mutual information\n67\nMutual Information (2/2)\nx\ny\nMutual information\n(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n(1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n0\n(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n(10, 9, 8, 7, 6, 5, 4, 3, 2, 1)\n3.322\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Mutual Information (2/2)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ If the two sets of values are completely independent\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– i.e., the value of one tells us nothing about the other\n– Then their mutual information is 0\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ If the two sets of values are completely dependent\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– i.e., knowing the value of one tells us the value of the other\n– Then they have maximum mutual information\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "x\ny\nMutual information\n(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n(1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n(10, 9, 8, 7, 6, 5, 4, 3, 2, 1)\n0\n3.322\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "67\n"
			}
		]
	},
	"68": {
		"page_id": 68,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Measure the average information in a single set of values\n– X: a set of values with m distinct values u1, u2, …, um\n– H(X): the entropy of X\n– P(X = uj): the probability of uj in X\n– I(X = uj): the amount of information acquired through observing uj\n• I(X = uj) = log2(1/P(X = uj)) = –log2P(X = uj)\n• As P(X = uj) increases, I(X = uj) decreases, and vice versa\nEntropy\n68\n𝐻 𝑋 = ෍\n𝑗=1\n𝑚\n𝑃 𝑋 = 𝑢𝑗 𝐼 𝑋 = 𝑢𝑗 = − ෍\n𝑗=1\n𝑚\n𝑃 𝑋 = 𝑢𝑗 log2 𝑃 𝑋 = 𝑢𝑗\nEntropy = 0\nEntropy = 0.81\nEntropy = 1\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Entropy\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Measure the average information in a single set of values\n"
			},
			{
				"audio_url": "",
				"font_size": 13,
				"text": "𝑚\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "𝐻 𝑋 = ෍\n"
			},
			{
				"audio_url": "",
				"font_size": 13,
				"text": "𝑗=1\n𝑚\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "𝑃 𝑋 = 𝑢𝑗 𝐼 𝑋 = 𝑢𝑗 = − ෍\n"
			},
			{
				"audio_url": "",
				"font_size": 13,
				"text": "𝑗=1\n𝑃 𝑋 = 𝑢𝑗 log2 𝑃 𝑋 = 𝑢𝑗\n– X: a set of values with m distinct values u1, u2, …, um\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– H(X): the entropy of X\n– P(X = uj): the probability of uj in X\n"
			},
			{
				"audio_url": "",
				"font_size": 13,
				"text": "– I(X = uj): the amount of information acquired through observing uj\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• I(X = uj) = log2(1/P(X = uj)) = –log2P(X = uj)\n• As P(X = uj) increases, I(X = uj) decreases, and vice versa\nEntropy = 0\nEntropy = 0.81\nEntropy = 1\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "68\n"
			}
		]
	},
	"69": {
		"page_id": 69,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Consider two sets of values, X and Y, which occur in pairs (X, Y)\n▪ First, we measure the average information (i.e., entropy) of X, Y, \nand (X, Y), respectively\nDefinition: Mutual Information (1/2)\n69\nX\n(1, 2, 3, 1, 3)\nY\n(2, 3, 1, 2, 2) \n(X, Y)\n((1, 2), (2, 3), (3, 1), (1, 2), (3, 2))\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Definition: Mutual Information (1/2)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Consider two sets of values, X and Y, which occur in pairs (X, Y)\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "X\nY\n(1, 2, 3, 1, 3)\n(2, 3, 1, 2, 2) \n(X, Y)\n((1, 2), (2, 3), (3, 1), (1, 2), (3, 2))\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ First, we measure the average information (i.e., entropy) of X, Y, \nand (X, Y), respectively\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "69\n"
			}
		]
	},
	"70": {
		"page_id": 70,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Finally, we obtain the mutual information of X and Y as follows:\n▪ The mutual information quantifies the “amount of information” \nobtained about X by observing Y, and vise versa\n– Note that I(X, Y) is symmetric, i.e., I(X, Y) = I(Y, X) \nDefinition: Mutual Information (2/2)\n70\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Definition: Mutual Information (2/2)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Finally, we obtain the mutual information of X and Y as follows:\n▪ The mutual information quantifies the “amount of information” \nobtained about X by observing Y, and vise versa\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Note that I(X, Y) is symmetric, i.e., I(X, Y) = I(Y, X) \n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "70\n"
			}
		]
	},
	"71": {
		"page_id": 71,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ Suppose we have two sets of values x and y to compare\n– x = (–3, –2, –1, 0, 1, 2, 3), y = (9, 4, 1, 0, 1, 4, 9)\n– Although there is a relationship yk = xk\n2, their correlation is 0\n▪ However, their mutual information I(x, y) = 1.9502\n– I(x, y) = H(x) + H(y) – H(x, y) = 2.8074 + 1.9502 – 2.8074\n(Ex) Mutual Information\n71\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "(Ex) Mutual Information\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ Suppose we have two sets of values x and y to compare\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– x = (–3, –2, –1, 0, 1, 2, 3), y = (9, 4, 1, 0, 1, 4, 9)\n"
			},
			{
				"audio_url": "",
				"font_size": 13,
				"text": "– Although there is a relationship yk = xk\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "2, their correlation is 0\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ However, their mutual information I(x, y) = 1.9502\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– I(x, y) = H(x) + H(y) – H(x, y) = 2.8074 + 1.9502 – 2.8074\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "71\n"
			}
		]
	},
	"72": {
		"page_id": 72,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ (Issue 1) Standardization\n– If attributes have different scales, standardize them to avoid being \ndominated by attributes with large values\n– Rescaling \n• Rescale the range of attributes to be [0, 1]\n– Mean normalization\n• Rescale based on the distance from the mean\n– Standardization (in statistics)\n• Make attributes have 0-mean and 1-variance\nIssues in Proximity Calculation (1/2)\n72\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Issues in Proximity Calculation (1/2)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ (Issue 1) Standardization\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– If attributes have different scales, standardize them to avoid being \ndominated by attributes with large values\n– Rescaling \n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• Rescale the range of attributes to be [0, 1]\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Mean normalization\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• Rescale based on the distance from the mean\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– Standardization (in statistics)\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• Make attributes have 0-mean and 1-variance\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "72\n"
			}
		]
	},
	"73": {
		"page_id": 73,
		"full_text": {
			"audio_url": "",
			"full_text": "▪ (Issue 2) Using weights\n– In some cases, some attributes are more important than others\n• (ex) When comparing two people, Age may be more important than Height\n– We can assign each attribute a different weight wk\n– The definition of the Minkowski distance can also be modified as follows:\nIssues in Proximity Calculation (2/2)\n73\nAttribute\nAge\nHeight\nWeight\nSalary\nWeight\n0.5\n0.2\n0.2\n0.1\n"
		},
		"text": [
			{
				"audio_url": "",
				"font_size": 32,
				"text": "Issues in Proximity Calculation (2/2)\n"
			},
			{
				"audio_url": "",
				"font_size": 24,
				"text": "▪ (Issue 2) Using weights\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– In some cases, some attributes are more important than others\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "• (ex) When comparing two people, Age may be more important than Height\n"
			},
			{
				"audio_url": "",
				"font_size": 13,
				"text": "– We can assign each attribute a different weight wk\n"
			},
			{
				"audio_url": "",
				"font_size": 18,
				"text": "Attribute\nWeight\nAge\n0.5\nHeight\nWeight\nSalary\n0.2\n0.2\n0.1\n"
			},
			{
				"audio_url": "",
				"font_size": 20,
				"text": "– The definition of the Minkowski distance can also be modified as follows:\n"
			},
			{
				"audio_url": "",
				"font_size": 14,
				"text": "73\n"
			}
		]
	}
}