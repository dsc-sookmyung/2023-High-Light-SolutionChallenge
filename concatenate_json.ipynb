{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\t\"K5\": {\n",
      "\t\t\"price\": \"5000\",\n",
      "\t\t\"year\": \"2015\"\n",
      "\t},\n",
      "\t\"Avante\": {\n",
      "\t\t\"price\": \"3000\",\n",
      "\t\t\"year\": \"2014\"\n",
      "\t}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "car_group = dict()\n",
    "\n",
    "\n",
    "k5 = dict()\n",
    "\n",
    "k5[\"price\"] = \"5000\"\n",
    "\n",
    "k5[\"year\"] = \"2015\"\n",
    "\n",
    "car_group[\"K5\"] = k5\n",
    "\n",
    "\n",
    "avante = dict()\n",
    "\n",
    "avante[\"price\"] = \"3000\"\n",
    "\n",
    "avante[\"year\"] = \"2014\"\n",
    "\n",
    "car_group[\"Avante\"] = avante\n",
    "\n",
    "\n",
    "#json 파일로 저장\n",
    "\n",
    "with open('./test.json', 'w', encoding='utf-8') as make_file:\n",
    "\n",
    "    json.dump(car_group, make_file, indent=\"\\t\")\n",
    "\n",
    "\n",
    "# 저장한 파일 출력하기\n",
    "\n",
    "with open('./test.json', 'r') as f:\n",
    "\n",
    "    json_data = json.load(f)\n",
    "\n",
    "print(json.dumps(json_data, indent=\"\\t\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fin\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import PIL.Image\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer, LTChar,LTLine,LAParams\n",
    "import os\n",
    "import json\n",
    "import PIL.Image\n",
    "import io\n",
    "import re\n",
    "\n",
    "\n",
    "path = r'./os.pdf'\n",
    "extract_data = {}\n",
    "pdf = fitz.open(path)\n",
    "page_id = 1\n",
    "cur_size = 0\n",
    "text = \"\"\n",
    "\n",
    "# full-text -> image 순으로 페이지당으로 나옴\n",
    "for i in range(len(pdf)):\n",
    "    each_page = {}\n",
    "    page = pdf[i]  # load page\n",
    "    images = page.get_images()\n",
    "    text = page.get_text('sentences')\n",
    "    \n",
    "    # for image in images:\n",
    "    #     base_img = pdf.extract_image(image[0])\n",
    "    #     image_data = base_img['image']\n",
    "    #     img = PIL.Image.open(io.BytesIO(image_data))\n",
    "    #     extension = base_img['ext']\n",
    "    #     img.save(\n",
    "    #         open(f\"temp_dataset/split_pages/image{page_id}.{extension}\", \"wb\"))\n",
    "    each_page[\"page_id\"] = page_id\n",
    "    full_text_data = {\"audio_url\": \"\", \"full_text\": text}\n",
    "    each_page[\"full_text\"] = full_text_data\n",
    "    extract_data[str(page_id)] = each_page\n",
    "    page_id += 1\n",
    "\n",
    "# line-text 페이지당으로 나옴       \n",
    "for page_layout in extract_pages(path):\n",
    "    each_page = {}\n",
    "    info = []\n",
    "    font_size = 0\n",
    "    for element in page_layout:\n",
    "        if isinstance(element, LTTextContainer):\n",
    "            for text_line in element:\n",
    "                for character in text_line:\n",
    "                    if isinstance(character, LTChar):\n",
    "                        next_size = round(character.size)\n",
    "                if (cur_size != 0) or (cur_size != next_size):\n",
    "                    cur_size = next_size\n",
    "                    text = \"\"\n",
    "                if cur_size == next_size:\n",
    "                    text += element.get_text()\n",
    "                info.append(\n",
    "                    {\"audio_url\": \"\", \"font_size\": cur_size, \"text\": text})\n",
    "            \n",
    "    extract_data[str(page_layout.pageid)][\"text\"] = info\n",
    "\n",
    "# 정확도 높이기\n",
    "# 줄바꿈 기준 쪼개고 글씨크기 기준으로 정확히 나누기\n",
    "for i in range(1, len(extract_data) + 1):\n",
    "    each_page_size = len(extract_data[str(i)][\"text\"])\n",
    "    each_page = extract_data[str(i)][\"text\"]\n",
    "    for j in range(each_page_size - 1):\n",
    "        cur_text = each_page[j][\"text\"]\n",
    "        next_text = each_page[j + 1][\"text\"]\n",
    "        if cur_text == next_text:\n",
    "            split_list = each_page[j][\"text\"].split(\"\\n\")\n",
    "            info = []\n",
    "            for k in range(len(split_list)):\n",
    "                font_size = each_page[j + k][\"font_size\"]\n",
    "                text = split_list[k] + \"\\n\"\n",
    "                text = re.sub(r\"[^\\w\\s]]\", \"\", text)  # import re\n",
    "                if text == \"\\n\":\n",
    "                    continue\n",
    "                else:\n",
    "                    each_page[j + k] = {\"audio_url\": \"\",\n",
    "                                        \"font_size\": font_size, \"text\": text}\n",
    "# with open('./split.json', 'w', encoding='utf-8') as make_file:\n",
    "#     json.dump(extract_data, make_file, indent=\"\\t\", ensure_ascii=False)\n",
    "# print(\"fin\")\n",
    "concat_text = \"\"\n",
    "# 글씨 크기 같은 애들은 묶기\n",
    "for i in range(1, len(extract_data) + 1):\n",
    "    each_page_size = len(extract_data[str(i)][\"text\"])\n",
    "    each_page = extract_data[str(i)][\"text\"]\n",
    "    concat_text = each_page[0][\"text\"]\n",
    "    to_del_list = []\n",
    "    for j in range(each_page_size - 1):\n",
    "        # j = 0\n",
    "        # while j != each_page_size - 1:\n",
    "        cur_size = each_page[j][\"font_size\"]\n",
    "        next_size = each_page[j + 1][\"font_size\"]\n",
    "        if cur_size == next_size:\n",
    "            concat_text += each_page[j + 1][\"text\"]\n",
    "            to_del_list.append(j)\n",
    "            # del(each_page[j + 1])\n",
    "        else:\n",
    "            each_page[j][\"text\"] = concat_text\n",
    "            concat_text = each_page[j + 1][\"text\"]\n",
    "            # to_del_list.append(j - 1)\n",
    "            j += 1\n",
    "    # print(to_del_list)\n",
    "    # 반복문을 통해서 리스트 뒤에서부터 지우기\n",
    "    # 스택이나 큐를 사용해서 제거\n",
    "    list_len = len(to_del_list)\n",
    "    for j in range(list_len - 1, -1, -1):\n",
    "        del(each_page[to_del_list[j]])\n",
    "    #     print(to_del_list[j])\n",
    "    # print()\n",
    "\n",
    "\n",
    "# print(extract_data)\n",
    "with open('./concat.json', 'w', encoding='utf-8') as make_file:\n",
    "    json.dump(extract_data, make_file, indent=\"\\t\", ensure_ascii=False)\n",
    "print(\"fin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fin\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "with open('./sample_text_only.json', 'r', encoding=\"utf-8\") as f:\n",
    "\n",
    "    extract_data = json.load(f)\n",
    "\n",
    "# print(json.dumps(json_data, indent=\"\\t\", ensure_ascii=False))\n",
    "\n",
    "# 정확도 높이기\n",
    "# 줄바꿈 기준 쪼개고 글씨크기 기준으로 정확히 나누기\n",
    "for i in range(1, len(extract_data) + 1):\n",
    "    each_page_size = len(extract_data[str(i)][\"text\"])\n",
    "    each_page = extract_data[str(i)][\"text\"]\n",
    "    for j in range(each_page_size - 1):\n",
    "        cur_text = each_page[j][\"text\"]\n",
    "        next_text = each_page[j + 1][\"text\"]\n",
    "        if cur_text == next_text:\n",
    "            split_list = each_page[j][\"text\"].split(\"\\n\")\n",
    "            info = []\n",
    "            for k in range(len(split_list)):\n",
    "                font_size = each_page[j + k][\"font_size\"]\n",
    "                text = split_list[k] + \"\\n\"\n",
    "                text = re.sub(r\"[^\\w\\s]]\", \"\", text) # import re\n",
    "                if text == \"\\n\":\n",
    "                    continue\n",
    "                else:\n",
    "                    each_page[j + k] = {\"audio_url\":\"\", \"font_size\": font_size, \"text\": text}\n",
    "# with open('./split.json', 'w', encoding='utf-8') as make_file:\n",
    "#     json.dump(extract_data, make_file, indent=\"\\t\", ensure_ascii=False)\n",
    "# print(\"fin\")\n",
    "concat_text = \"\"        \n",
    "# 글씨 크기 같은 애들은 묶기\n",
    "for i in range(1, len(extract_data) + 1):\n",
    "    each_page_size = len(extract_data[str(i)][\"text\"])\n",
    "    each_page = extract_data[str(i)][\"text\"]\n",
    "    concat_text = each_page[0][\"text\"]\n",
    "    to_del_list = []\n",
    "    for j in range(each_page_size - 1):\n",
    "    # j = 0\n",
    "    # while j != each_page_size - 1:\n",
    "        cur_size = each_page[j][\"font_size\"]\n",
    "        next_size = each_page[j + 1][\"font_size\"]\n",
    "        if cur_size == next_size:\n",
    "            concat_text += each_page[j + 1][\"text\"]\n",
    "            to_del_list.append(j)\n",
    "            # del(each_page[j + 1])\n",
    "        else:\n",
    "            each_page[j][\"text\"] = concat_text\n",
    "            concat_text = each_page[j + 1][\"text\"]\n",
    "            # to_del_list.append(j - 1)\n",
    "            j += 1\n",
    "    # print(to_del_list)\n",
    "    # 반복문을 통해서 리스트 뒤에서부터 지우기\n",
    "    # 스택이나 큐를 사용해서 제거\n",
    "    list_len = len(to_del_list)\n",
    "    for j in range(list_len - 1, -1, -1):\n",
    "        del(each_page[to_del_list[j]])\n",
    "    #     print(to_del_list[j])\n",
    "    # print()\n",
    "        \n",
    "    \n",
    "# print(extract_data)\n",
    "with open('./concat.json', 'w', encoding='utf-8') as make_file:\n",
    "    json.dump(extract_data, make_file, indent=\"\\t\", ensure_ascii=False)\n",
    "print(\"fin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "temp = [1, 2, 3, 4, 5, 6]\n",
    "for i in range(len(temp), 0, -1):\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35e0334895c7257ffa902a82750dbe09ec85e290cbc1321a3819ebf1c9410545"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
